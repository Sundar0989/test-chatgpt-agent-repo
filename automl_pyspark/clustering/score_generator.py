"""
Clustering Score Generator

Generates scoring scripts for clustering models.
"""

import os
from typing import Dict, List, Any


class ClusteringScoreGenerator:
    """Generates scoring scripts for clustering models."""
    
    def __init__(self, output_dir: str, user_id: str, model_id: str, model_literal: str):
        self.output_dir = output_dir
        self.user_id = user_id
        self.model_id = model_id
        self.model_literal = model_literal
        
        # Model type to class name mappings
        self.model_class_names = {
            'kmeans': 'KMeansClusteringScorer',
            'bisecting_kmeans': 'BisectingKMeansClusteringScorer',
            'gaussian_mixture': 'GaussianMixtureClusteringScorer',
            'dbscan': 'DBSCANClusteringScorer'
        }
        
        # Model type to display name mappings
        self.model_display_names = {
            'kmeans': 'K-Means Clustering',
            'bisecting_kmeans': 'Bisecting K-Means Clustering',
            'gaussian_mixture': 'Gaussian Mixture Clustering',
            'dbscan': 'DBSCAN Clustering'
        }
    
    def generate_scoring_code(self, config: Dict[str, Any], feature_vars: List[str],
                            selected_vars: List[str], categorical_vars: List[str],
                            numerical_vars: List[str], best_model_type: str):
        """Generate scoring code for clustering models."""
        print(f"üìù Generating clustering scoring code for {best_model_type}")
        
        # Get clustering configuration
        clustering_config = config.get('clustering', {})
        models_config = clustering_config.get('models', {})
        
        # Generate scripts for enabled model types
        model_types = ['kmeans', 'bisecting_kmeans', 'gaussian_mixture', 'dbscan']
        
        scripts_generated = 0
        for model_type in model_types:
            if models_config.get(f'run_{model_type}', False):
                self._generate_model_scoring_script(
                    model_type, config, feature_vars, selected_vars,
                    categorical_vars, numerical_vars
                )
                scripts_generated += 1
        
        if scripts_generated > 0:
            print(f"‚úÖ Generated {scripts_generated} clustering scoring scripts in {self.output_dir}")
        else:
            print("‚ö†Ô∏è  No clustering scoring scripts generated - no models were enabled")
    
    def _generate_model_scoring_script(self, model_type: str, config: Dict[str, Any],
                                     feature_vars: List[str], selected_vars: List[str],
                                     categorical_vars: List[str], numerical_vars: List[str]):
        """Generate scoring script for a specific clustering model type."""
        
        class_name = self.model_class_names.get(model_type, f'{model_type.title()}ClusteringScorer')
        display_name = self.model_display_names.get(model_type, f'{model_type.title()} Clustering')
        
        script_content = self._create_script_template(
            model_type, class_name, display_name, config,
            feature_vars, selected_vars, categorical_vars, numerical_vars
        )
        
        # Save the script
        script_filename = f"{model_type}_clustering_scorer.py"
        script_path = os.path.join(self.output_dir, script_filename)
        
        try:
            with open(script_path, 'w') as f:
                f.write(script_content)
            print(f"   üìÑ Generated: {script_filename}")
        except Exception as e:
            print(f"   ‚ùå Error generating {script_filename}: {str(e)}")
    
    def _create_script_template(self, model_type: str, class_name: str, display_name: str,
                               config: Dict[str, Any], feature_vars: List[str],
                               selected_vars: List[str], categorical_vars: List[str],
                               numerical_vars: List[str]) -> str:
        """Create the scoring script template for clustering models."""
        
        # Generate the script content
        script = f'''"""
{display_name} Model Scoring Script

This script provides clustering scoring functionality for the {model_type} model.
Generated by AutoML PySpark Package

Usage:
    scorer = {class_name}()
    cluster_results = scorer.score_data('path/to/new_data.csv')
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
from pyspark.sql.functions import col, when, isnan, isnull, lit
from pyspark.sql.types import DoubleType
from pyspark.ml.clustering import KMeansModel, BisectingKMeansModel, GaussianMixtureModel

# For DBSCAN (sklearn-based implementation)
try:
    from sklearn.cluster import DBSCAN
    from sklearn.preprocessing import StandardScaler
    import numpy as np
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
from pyspark.ml.evaluation import ClusteringEvaluator

# Parameters
user_id = '{self.user_id}'
mdl_output_id = '{self.model_id}'
mdl_ltrl = '{self.model_literal}'

# Model variables - These will be loaded from model_info.json
SELECTED_VARS = {selected_vars}
CATEGORICAL_VARS = {categorical_vars}
NUMERICAL_VARS = {numerical_vars}
IMPUTE_VALUE = {config.get('impute_value', 0.0)}

class {class_name}:
    """
    {display_name} Model Scorer class for clustering new data.
    """
    
    def __init__(self, spark_session: SparkSession = None):
        """
        Initialize the clustering scorer.
        
        Args:
            spark_session: PySpark SparkSession (optional)
        """
        self.spark = spark_session or self._create_spark_session()
        self.preprocessing_pipeline = None
        self.clustering_model = None
        self.char_labels = None
        self.loaded = False
        
    def _create_spark_session(self) -> SparkSession:
        """Create PySpark session with optimized configuration for clustering scoring."""
        return SparkSession.builder \
            .appName("{display_name} Scorer") \
            .master("local[*]") \
            .config("spark.driver.bindAddress", "127.0.0.1") \
            .config("spark.driver.host", "127.0.0.1") \
            .config("spark.driver.memory", "4g") \
            .config("spark.executor.memory", "4g") \
            .config("spark.sql.adaptive.enabled", "false") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "false") \
            .config("spark.sql.adaptive.skewJoin.enabled", "false") \
            .config("spark.sql.adaptive.localShuffleReader.enabled", "false") \
            .config("spark.local.dir", "/tmp") \
            .config("spark.sql.warehouse.dir", "/tmp/spark-warehouse") \
            .getOrCreate()
    
    def load_model(self, model_dir: str = "."):
        """
        Load the trained clustering model and preprocessing pipeline.
        
        Args:
            model_dir: Directory containing the saved model files
        """
        print(f"Loading {model_type} clustering model from {{model_dir}}")
        
        try:
            # Load categorical encoder first (consistent with regression/classification)
            encoder_path = os.path.join(model_dir, "char_labels")
            if os.path.exists(encoder_path):
                from pyspark.ml import PipelineModel
                self.char_labels = PipelineModel.load(encoder_path)
                print("   ‚úÖ Categorical encoder loaded successfully")
            else:
                print(f"   ‚ö†Ô∏è  Categorical encoder not found at {{encoder_path}}")
                print("   üîÑ Will use basic categorical handling")
                self.char_labels = None
            
            # Load preprocessing pipeline
            pipeline_path = os.path.join(model_dir, "preprocessing_pipeline")
            if os.path.exists(pipeline_path):
                self.preprocessing_pipeline = PipelineModel.load(pipeline_path)
                print("   ‚úÖ Preprocessing pipeline loaded successfully")
            else:
                print(f"   ‚ö†Ô∏è  Preprocessing pipeline not found at {{pipeline_path}}")
                print("   üîÑ Using basic preprocessing fallback")
            
            # Load clustering model
            model_path = os.path.join(model_dir, f"{model_type}_model")
            if os.path.exists(model_path):
                if "{model_type}" == "kmeans":
                    self.clustering_model = KMeansModel.load(model_path)
                elif "{model_type}" == "bisecting_kmeans":
                    self.clustering_model = BisectingKMeansModel.load(model_path)
                elif "{model_type}" == "gaussian_mixture":
                    self.clustering_model = GaussianMixtureModel.load(model_path)
                elif "{model_type}" == "dbscan":
                    # DBSCAN models are saved as pickle files with sklearn
                    import pickle
                    with open(model_path, 'rb') as f:
                        self.clustering_model = pickle.load(f)
                print(f"   ‚úÖ {model_type} clustering model loaded")
            else:
                raise FileNotFoundError(f"Clustering model not found at {{model_path}}")
            
            self.loaded = True
            print("‚úÖ Model loading completed")
            
        except Exception as e:
            print(f"‚ùå Error loading model: {{str(e)}}")
            raise
    
    def preprocess_data(self, data):
        """
        Apply preprocessing to new data.
        
        Args:
            data: Input DataFrame
            
        Returns:
            Preprocessed DataFrame with features column
        """
        if isinstance(data, str):
            # Load data from file path
            data = self.spark.read.csv(data, header=True, inferSchema=True)
        
        # First apply categorical encoding if available
        if self.char_labels:
            print("   üîÑ Applying categorical encoding...")
            data = self.char_labels.transform(data)
            print("   ‚úÖ Categorical encoding applied")
        
        # Then apply preprocessing pipeline if available
        if self.preprocessing_pipeline:
            processed_data = self.preprocessing_pipeline.transform(data)
        else:
            # Basic preprocessing fallback
            print("   Using basic preprocessing (no pipeline found)")
            
            # Select feature columns
            feature_cols = [col for col in data.columns if col in SELECTED_VARS]
            
            # Handle missing values
            for col_name in feature_cols:
                data = data.withColumn(col_name, 
                                     when(col(col_name).isNull() | isnan(col(col_name)), 
                                          lit(IMPUTE_VALUE)).otherwise(col(col_name)))
            
            # Create features vector (basic implementation)
            from pyspark.ml.feature import VectorAssembler
            assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
            processed_data = assembler.transform(data)
        
        return processed_data
    
    def score_data(self, input_data, output_path: str = None):
        """
        Score new data using the clustering model.
        
        Args:
            input_data: Input data (file path or DataFrame)
            output_path: Optional path to save results
            
        Returns:
            DataFrame with cluster assignments and metrics
        """
        if not self.loaded:
            raise ValueError("Model not loaded. Call load_model() first.")
        
        print("üîç Scoring new data with clustering model...")
        
        # Preprocess the data
        print("   üìä Preprocessing data...")
        processed_data = self.preprocess_data(input_data)
        
        # Apply clustering
        print(f"   ü§ñ Applying {model_type} clustering...")
        
        if "{model_type}" == "dbscan":
            # DBSCAN uses a custom transform method
            cluster_results = self._apply_dbscan_clustering(processed_data)
        else:
            # Standard PySpark clustering models
            cluster_results = self.clustering_model.transform(processed_data)
        
        # Calculate clustering metrics
        print("   üìà Calculating clustering metrics...")
        metrics = self._calculate_metrics(cluster_results)
        
        # Add cluster statistics
        cluster_summary = self._get_cluster_summary(cluster_results)
        
        # Show results summary
        print(f"\\nüìä Clustering Results Summary:")
        print(f"   ‚Ä¢ Total data points: {{cluster_results.count():,}}")
        print(f"   ‚Ä¢ Number of clusters: {{metrics.get('num_clusters', 'N/A')}}")
        print(f"   ‚Ä¢ Silhouette score: {{metrics.get('silhouette_score', 'N/A'):.4f}}")
        
        print(f"\\nüéØ Cluster Distribution:")
        cluster_summary.show()
        
        # Save results if requested
        if output_path:
            print(f"üíæ Saving results to {{output_path}}")
            cluster_results.write.mode("overwrite").option("header", True).csv(output_path)
        
        return {{
            'cluster_assignments': cluster_results,
            'metrics': metrics,
            'cluster_summary': cluster_summary
        }}
    
    def _calculate_metrics(self, cluster_results):
        """Calculate clustering quality metrics."""
        try:
            evaluator = ClusteringEvaluator(featuresCol="features", predictionCol="prediction")
            silhouette_score = evaluator.evaluate(cluster_results)
            
            num_clusters = cluster_results.select("prediction").distinct().count()
            
            return {{
                'silhouette_score': silhouette_score,
                'num_clusters': num_clusters,
                'total_points': cluster_results.count()
            }}
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Could not calculate metrics: {{str(e)}}")
            return {{'silhouette_score': None, 'num_clusters': None}}
    
    def _get_cluster_summary(self, cluster_results):
        """Get summary statistics for each cluster."""
        return cluster_results.groupBy("prediction") \
            .count() \
            .withColumnRenamed("prediction", "cluster") \
            .withColumnRenamed("count", "num_points") \
            .orderBy("cluster")
    
    def _apply_dbscan_clustering(self, processed_data):
        """Apply DBSCAN clustering with custom transform method."""
        if not SKLEARN_AVAILABLE:
            raise ImportError("DBSCAN requires sklearn. Install with: pip install scikit-learn")
        
        # Extract features from PySpark DataFrame
        features_list = []
        for row in processed_data.select("features").collect():
            features_list.append(row.features.toArray())
        
        features_array = np.array(features_list)
        
        # Scale features using the fitted scaler from the model
        if hasattr(self.clustering_model, 'scaler'):
            features_scaled = self.clustering_model.scaler.transform(features_array)
        else:
            # Fallback: use StandardScaler
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features_array)
        
        # Predict clusters
        predictions = self.clustering_model.model.predict(features_scaled)
        
        # Convert back to PySpark DataFrame format
        from pyspark.sql.types import StructType, StructField, IntegerType
        from pyspark.sql import Row
        
        prediction_rows = [Row(prediction=int(pred)) for pred in predictions]
        prediction_df = self.spark.createDataFrame(prediction_rows, StructType([StructField("prediction", IntegerType(), False)]))
        
        # Join with original data
        result = processed_data.crossJoin(prediction_df.limit(processed_data.count()))
        return result

# Example usage
if __name__ == "__main__":
    # Initialize scorer
    scorer = {class_name}()
    
    # Load the trained model
    # scorer.load_model("path/to/your/model/directory")
    
    # Score new data
    # results = scorer.score_data("path/to/your/new_data.csv")
    
    print("\\nüí° Usage Instructions:")
    print("1. Initialize the scorer: scorer = {class_name}()")
    print("2. Load your trained model: scorer.load_model('path/to/model')")
    print("3. Score new data: results = scorer.score_data('new_data.csv')")
    print("4. Access results: results['cluster_assignments'], results['metrics']")
'''
        
        return script 